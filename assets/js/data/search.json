[ { "title": "Kafka Consumers in Spring Boot", "url": "/posts/KafkaConsumers/", "categories": "Kafka", "tags": "kafka, consumers", "date": "2022-11-03 00:00:00 +0000", "snippet": "Kafka ConsumersSpring boot creates Kafka consumers by adding annotation on @KafkaListener, simple example of a consumer is:@Componentpublic class SimpleKafkaConsumer1 { private static final Logger LOGGER = LoggerFactory.getLogger(SimpleKafkaConsumer1.class); private CountDownLatch latch = new CountDownLatch(1); private String payload; @KafkaListener( topics = AlertKafkaProducer.POJO_TOPIC, groupId = \"2\" ) public void receiveAlertDTO(ConsumerRecord&lt;String, AlertDTO&gt; consumerRecord) { LOGGER.info(\"received receiveAlertDTO='{}', groupId = \\\"2\\\"\", consumerRecord.toString()); payload = consumerRecord.toString(); latch.countDown(); }}The current code will create a bean and will reweave messages that are objects from some class AlertDTO.The KafkaListener annotation also specifies two params, a groupId and a topic. The topic is equivalent to DB tablein a relational database like MySQL, it is Kafka’s logical unit for storing and retreating data. Messages arepublished and consumed inside a topic. The “GroupId” concept does not exist in relational databases. It groups together moreconsumers to allow parallel consumption of the topic’s messages. The level of parallelism in the consumption of the topic’smessages is controlled by the number of partitions the topic has. Partitions for a topic are defined when a topic iscreated. Every partition is assigned to a consumer inside every consumer’s group. If a consumer group has oneconsumer then all the partitions in the topic will be assigned to that consumer. If there are more consumers in aconsumer group then the partitions of a topic will be shared between the consumers. In any case, one partition can’t havemore than one consumer assigned. If there are more consumers than partitions, some consumers will be idle, but allpartitions will have a consumer assigned. Once a message is consumed or in other words, once a consumer reads a message,the action of consumption is recorded and Kafka holds a record of the consumed messages for every consumer in everyconsumer group.Going back to java-spring, more beans can be created to create more consumes or also more listeners in the same bean.If not specified explicitly consumers are randomly assigned to partitions. It is essential to know that consumers will notdistinguish between the object that they need to consume.The following examples define two consumers. The first one will consume the record from type “String” and the second one from“SomeObject”.@KafkaListener(topics = \"POJO_TOPIC\",groupId = \"2\")public void receiveString(ConsumerRecord&lt;String, String&gt; consumerRecord) { LOGGER.info(\"received receiveAlertDTO='{}', groupId = \\\"2\\\"\", consumerRecord.toString());}@KafkaListener(topics = \"POJO_TOPIC\",groupId = \"2\")public void receiveSomeObject(ConsumerRecord&lt;String, SomeObject&gt; consumerRecord) { LOGGER.info(\"received receiveAlertDTO='{}', groupId = \\\"2\\\"\", consumerRecord.toString());}*!!This will not work!!**. Consumers are not assigned to the type of objects they consume!Key takeaway Consumers are organized in consumer groups. Consumers from different groups are reading the topic independently and in parallel. Consumers from same group read the topic in parallel only if they are assigned to different partitions. Kafka keeps track of what consumers have already read, this value is called consumer group offset. Spring does not assign consumers on the type of data they consume, or read. Consumers are assigned to partitionsindependently if they are not explicitly declared by the developer." }, { "title": "Kafka Topic", "url": "/posts/KafkaTopic/", "categories": "Kafka", "tags": "kafka, topic", "date": "2022-09-14 00:00:00 +0000", "snippet": "Kafka TopicPlease note that in the following text, consume and read are synonyms in the context of Kafka topic as are write and produce.Is a logical concept that holds, and stores messages. Clients are storing and reading messages from a Kafka topic.The topic is a non-concrete concept, topic is not a physical structure. Topic is an abstraction of the Kafka storage system,similar to what a database table is in a relational database. One topic is usually fiscally distributed between differentBrokers in fiscal blocks called partitions that are actually folders in thebroker’s file system. Topics can also be backed up/copied several times on the brokers. The number of copies that atopic has is called the replication factor. Topics are used by the client’s applications, the producer, and consumerapplications. Application developers need to have an understanding of what a topic is and how to produce and consumemessages from a topic. For example, an API application might write/publish messages to Kafka topic or it mightread/consume messages from a Kafka topic. That is why it is said that Kafka topic is an abstraction of the Kafkastorage system. Applications that use Kafka are not aware, or interested in how data is stored in Kafka, they only knowhow to read/consume or write/produce data/messages to the Kafka topic.Kafka topic, reading/consuming and writing/producing messages conceptKafka messages are generally written and read in an orderedway. The first message that is written is generally the first message that is read. Off-course theapplication can ask to read a particular message skipping the “First in First out” concept. For example, consumers can onlyread the last 100 messages, or they can start reading from the first message or only start reading the new upcomingmessages, without caring about the previous messages.Parallel and sequential message reading in Kafka topicKafka can read topic messages in parallel and sequentially. Parallel reading is possible if the messages are independentby nature or business case, but it happens that some messages are required to be read sequentially, one after the other.Let’s say transactions from the user’s bank accounts. The order of statements in a bank account must be sequential by its business nature and requirements.These transactions must be ordered by date, if they are not, then the account status might be wrong so the reading of transaction messages for a client must be sequential. On the other hand, it canhappen that messages reading and writing can be parallelized. Let’s take the example of calculating the account balance in two different bank accounts.To achieve this parallel writing and reading a topic can be split into partitions. Every message within the topic’s partition will be consumed or produced sequentiallybut messages from two or more partitions can be consumed and or produced in parallel. Therefore in conclusion, if a system with sequential read and write needs to be developed then a topic with one partition needs to be created if a parallel system is required then a topic can be split into partitions that can work in parallel. Partitions are the building blocks of topics and they are physically present on the Brokers. Important is that,Reading and writing on different topics is always independentKafka topic, important commandsCreate a topic hello_world:kafka-topics --create --topic hello_world --partitions 3 --replication-factor 1 --bootstrap-server localhost:9092Send message to hello_world:kafka-console-producer --topic hello_world --broker-list localhost:9092Key takeawayKafka topic is an abstraction of the Kafka storage system.Kafka topic is physically divided into partitions located on the same or different broker. Partitions are folders in thebroker file system that message data in files called segments.Kafka topic has a replication factor, that is the number of copies of that topic in the Kafka system or cluster.Messages in the same partition are produced and consumed sequentially, but independently between different partitionsof the same topic." }, { "title": "Kafka Brokers", "url": "/posts/KafkaBrokers/", "categories": "Kafka", "tags": "kafka, brokers, zookeeper", "date": "2022-09-14 00:00:00 +0000", "snippet": "Kafka BrokersKafka is composed of two different clusters, Brokers cluster and ZooKeeper cluster.Note: There is an idea to replace ZooKeeper with own managed quorum.Before starting the broker cluster the Zookeeper cluster needs to be running.ZookeeperZookeeper cluster is used for orchestration/management of the brokers in the Broker cluster. This implies that the brokersrequire running zookeeper already running. Here is a simpledocker-compose file containing one Zookeeper node and threebrokers. The brokers have the task to store the data produced by the producer clients and provide the data for consumingto the consumer clients. When a consumer or a producer client wants to either store/produce data or consume data,at start it will ask any of the cluster node for the address and port of the broker that will handle the data.On the second step the consumer aether reads or writes the data to the broker.BrokerIs an instance that has the purpose to store the messages delivered from the produces and provide themessages to the consumers. Messages are stored in topics that are divided into partitions and at theend partitions are divided into segments. Topic is a logical concept, it is a logical placewhere producers write and consumers read. Topics internally are made from one or more partitions. The partitions arestored on more than one brokers. It could be that the same portion can be copied on more than one broker, or differentpartitions from the same topic are distributed between different brokers. Thereforone topic can be divided between more brokers. While topic is a logical concept, partitions and segmentsare physical folders and files accordingly, and are located on the broker file system.Brokers are instances that hold the partitions and the segments. Brokers are the ‘database’ of the kafka system." }, { "title": "Simple Kafka docker-compose explained", "url": "/posts/KafkaWithDockerCompose/", "categories": "Kafka", "tags": "TAG", "date": "2022-06-15 00:00:00 +0000", "snippet": "Prerequisites Installed docker and docker-compose Optional, (Offset Explorer (formerly Kafka Tool))[https://www.kafkatool.com/download.html] UI tool to view the content of the kafka system that includes the nodes, topics, and consumersSimple Kafka docker-compose explainedThe best way to start learning and using Kafka is with docker, docker-compose. The simplest configuration of Kafka will be to have one zookeeper node and one broker node in a docker-compose file but most of the examples in the books and tutorials teach Kafka in an environment with one Zookeeper node and three broker nodes.The simplest docker-compose file will look like the following:version: '2'services: zookeeper: image: confluentinc/cp-zookeeper:latest environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 ports: - 22181:2181 broker1: image: confluentinc/cp-kafka:latest depends_on: - zookeeper ports: - 29092:29092 environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: INTERNAL_LISTENER://broker1:9092,EXTERNAL_LISTENER://localhost:29092 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL_LISTENER:PLAINTEXT,EXTERNAL_LISTENER:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL_LISTENER broker2: image: confluentinc/cp-kafka:latest depends_on: - zookeeper ports: - 29093:29093 environment: KAFKA_BROKER_ID: 2 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: INTERNAL_LISTENER://broker2:9092,EXTERNAL_LISTENER://localhost:29093 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL_LISTENER:PLAINTEXT,EXTERNAL_LISTENER:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL_LISTENER broker3: image: confluentinc/cp-kafka:latest depends_on: - zookeeper ports: - 29094:29094 environment: KAFKA_BROKER_ID: 3 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: INTERNAL_LISTENER://broker3:9092,EXTERNAL_LISTENER://localhost:29094 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL_LISTENER:PLAINTEXT,EXTERNAL_LISTENER:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL_LISTENERAs it can be noticed, all the brokers need different names, ports, KAFKA_BROKER_ID and appropriate KAFKA_ADVERTISED_LISTENERS.Optional using Offset explorerTo visually see and explore the Kafka cluster Offset Explorer can be used. Download from here depending on your platform. After installing set the following values in order to connect to the cluster: Properties -&gt; Name of cluster: Choose a name for your cluster Properties -&gt; Zookeeper Host: localhost Properties -&gt; Zookeeper Port: 22181 Advanced -&gt; Bootstrap Server: localhost:29092 (the external address of a broker(s)) Reference: https://rmoff.net/2018/08/02/kafka-listeners-explained/ https://www.baeldung.com/ops/kafka-docker-setup Kafka in Action by Dylan Scott, Viktor Gamov, Dave Klein" }, { "title": "Setting up WireGardVPN at home", "url": "/posts/WireGuardVPN/", "categories": "VPN, Wireguard", "tags": "TAG", "date": "2022-06-15 00:00:00 +0000", "snippet": "How to set-up WireGuardVPN on debian, ubuntu?In this tutorial the following software will be used, required: curl docker, docker compose pivpn duck-dns docker imageRunning vpn at home requires three general components: VPN server, this is a PC that is running VPN. In our case wireguard Dynamic DNS service, is a service that can connect forward internet traffic from a domain to an IP address. This is required since the vpn clients will use this domain as the url to connect to the VPN. The Dynamic DNS then will forward traffic from this url to the IP address that in this case is our PC running wireguard. Important point to note here is that the IP of a home PC is changing and ones the ip changes the Dynamic DNS will need to be informed of the new IP address. Port forwarding, the home router needs to forward the VPN traffic to the wireguard server PC. This process is called port forwarding.Setup wireguard on debian / ubuntuInstall curlAs first step you need to install curl, that is in case it is not already installed.sudo apt install curlReference: https://www.cyberciti.biz/faq/how-to-install-curl-command-on-a-ubuntu-linux/Install piVPNSecond step, piVPN needs to be installed. PiVPN is an easy way to set up wireguard. After running the following command curl -L https://install.pivpn.io | bash, step by step installation process is started that will guide you thew the install process. PiVPN also offers OpenVpn so make sure you pick the correct vpn during this process. Also pls select custom-dns from the dns list as this tutorial shows how to use Duck-dns.Reference: https://www.pivpn.io/Set up Dynamic DNSTo for dynamic dns DuckDNS will be used. This requires setting up account on DuckDNS and getting a domain. After you have a domain you need to periodically update the IP address associated with this domain.Get a domain on DuckDNSDomains on duck dns are free but are limited per account. Refresh Dynamic DNS IP addressThis requires periodically calling the DUCK DNS API from the PC that runs wireguard. To do this a script can be created, but there is also already existing docker image specially designed for this task that can be used. In this tutorial I’ll use docker-compose to run the image as I find it personally more convenient.Install dockercurl -fsSL https://get.docker.com -o get-docker.shsudo sh get-docker.shReference: https://docs.docker.com/engine/install/ubuntu/Install docker-composesudo apt-get install docker-compose-pluginCreate docker-compose fileOn the pc running wireguard, create a docker compose file docker-compose.yml with the following content:version: \"2.1\"services: duckdns: image: ghcr.io/linuxserver/duckdns container_name: duckdns environment: - PUID=1000 #optional - PGID=1000 #optional - TZ=Europe/XXXXX - SUBDOMAINS=oXXX-XXX - TOKEN=5187bc8b-XXXX-XXXX-XXXX-XXXXXXXXXXXX - LOG_FILE=false #optional restart: unless-stoppedRun docker-composeRun the following command on the same level of the docker-compose.yml file:docker-compose upor to run in background:docker-compose up -dReference: https://docs.docker.com/compose/gettingstarted/Port forwardingPort forwarding is the last step. The concept here is that wireguard used by default 51820 port. Most probably your router is blocking this port so you have to redirect any traffic cumming to port 51820 to the IP address of PC running wireguard. Port forwarding is a unique process for every router manufacture and also perhaps every router, thats why you will need to research for your specific model. Important thing here is to assign a static IP address to the pc running wireguard. This is important since In case the IP changes the port forwarding might not work any more.TestGet keys from for the wireguard clients, set them up and test the process. https://jamstackthemes.dev/demo/theme/jekyll-theme-chirpy/" } ]
